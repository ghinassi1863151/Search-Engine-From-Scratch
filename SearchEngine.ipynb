{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd777c56-1b81-4aad-8fd0-023e9d41026c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a Search Engine from scratch\n",
    "\n",
    "We want to build our own search engine from scratch by following this steps:\n",
    "    \n",
    "    1. Build our own datasets retrieving almost 20000 html pages referring to a list of ranked animes of all time.\n",
    "    \n",
    "    2. Preprocessing of the text\n",
    "    \n",
    "    3. Building the search engine to compute synopsis related queries\n",
    "    \n",
    "    4. Imporving the search engine computing the cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44f6c3-9986-42e6-b348-0c648ab7a489",
   "metadata": {},
   "source": [
    "## 0. Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2020b-6ede-404d-94e9-4455d78dae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f #import of all the functions we made for this homework.\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import csv\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320cf32-98ba-46bf-b2d4-d82f88308016",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data collection\n",
    "\n",
    "First we have to retrieve the urls with the BeautifulSoup this is an easy task. We give you the code but we'll provide ypu already with the processed pages it's not mandatory to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b3ef4-fea3-447d-a108-6147057f51fe",
   "metadata": {
    "id": "gKU4EAJN0PYV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "anime = []\n",
    "\n",
    "for page in tqdm(range(0, 383)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6090d13-10ae-4bd4-9188-6282294e9144",
   "metadata": {},
   "source": [
    "## Crawler\n",
    "In this part we'll download the html pages corresponding to the urls. It can be helpful to use a time sleep to avoid being blocked for too much requests. We stored the anime's html in folders dividing them by the correspondent page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5207b-041e-4e9e-83f6-398662a796f2",
   "metadata": {
    "id": "dcdv3C5Fj7P8"
   },
   "source": [
    "Once we get all the urls in the first 400 pages of the list, we:\n",
    "\n",
    "1. Download the html corresponding to each of the collected urls.\n",
    "2. After we collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, we will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900798c-c32d-4abc-80bb-f6b5c50a4a9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv3uqvu_CloF",
    "outputId": "28acea09-f65e-40b9-ab34-aa1085e616fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:00<00:00, 258.34it/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir \"./page_folders/\"\n",
    "import os\n",
    "for page in tqdm(range(1, 384)):\n",
    "    folder = \"page\"+str(page)\n",
    "    path = \"./page_folders/\"+folder\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d239c-4771-4f13-8096-cdc3b0cfaa3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V64-yngoAnc5",
    "outputId": "3d95414e-789f-4935-ae04-f7a259a2c322"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [30:37<00:00, 204.14s/it]\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(range(0, 383)): \n",
    "    folder = \"./page_folders/page\"+str(page+1)\n",
    "    update_page = 50*page\n",
    "    for i in range(0,50):   # 1 -> 50\n",
    "        url = f'{anime[update_page+i]}'\n",
    "        response = requests.get(url)   \n",
    "        filename = r\"\"+folder+\"/anime_\"+str(update_page+i+1)+\".html\"\n",
    "        with open(filename,'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ec02-c4db-49c8-bbfe-622b99b61bf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parser\n",
    "Once we have the html we want to collect some informations about the pages. We implemented a parsing function to collect for each anime the following informations:\n",
    "\n",
    "1. Anime Name (to save as animeTitle): String\n",
    "2. Anime Type (to save as animeType): String\n",
    "3. Number of episode (to save as animeNumEpisode): Integer\n",
    "4. Release and End Dates of anime (to save as releaseDate and endDate): convert both release and end date into datetime format.\n",
    "5. Number of members (to save as animeNumMembers): Integer\n",
    "6. Score (to save as animeScore): Float\n",
    "7. Users (to save as animeUsers): Integer\n",
    "8. Rank (to save as animeRank): Integer\n",
    "9. Popularity (to save as animePopularity): Integer\n",
    "10. Synopsis (to save as animeDescription): String\n",
    "11. Related Anime (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "12. Characters (to save as animeCharacters): List of strings.\n",
    "13. Voices (to save as animeVoices): List of strings\n",
    "14. Staff (to save as animeStaff): Include the staff name and their responsibility/task in a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394b691-ac45-4bf9-beaa-d22c5b5f9354",
   "metadata": {
    "id": "P3zT_O69lEWU"
   },
   "source": [
    "For each anime, we create an ***anime_i.tsv*** file of this structure:\n",
    "- animeTitle \\t animeType \\t  ... \\t animeStaff \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7de32-4508-49bd-8a55-91aa008b04e7",
   "metadata": {
    "id": "KPL4p1UWloDF"
   },
   "source": [
    "If an information is missing, we just leave it as an empty string. Example:\n",
    "\n",
    "- animeTitle \\t animeType \\t  ... \\t animeStaff\n",
    "    \n",
    "- Fullmetal Alchemist: Brotherhood \\t TV \\t ... \\t [['Cook, Justin', 'Producer'], ['Irie, Yasuhiro', 'Director, Episode Director, Storyboard'], ['Yonai, Noritomo', 'Producer'], ['Mima, Masafumi', 'Sound Director']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649aa5d1-0247-498b-916b-bbfe6c46ed1d",
   "metadata": {},
   "source": [
    "We create a folder to store all the tsv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2931696-2cad-4d64-a08e-dc807f3f36b7",
   "metadata": {
    "id": "mf1DjivS9pKo"
   },
   "outputs": [],
   "source": [
    "!mkdir \"./tsv_files/\"\n",
    "import os\n",
    "for page in tqdm(range(1, 384)):\n",
    "    folder = \"page\"+str(page)\n",
    "    path = \"./tsv_files/\"+folder\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826004ec-ac01-446d-963f-86ef14dfe825",
   "metadata": {},
   "source": [
    "Then we parse the html and convert them into tsv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d33006-8d7c-4ab2-b8bc-fd36547d9ec5",
   "metadata": {
    "id": "4haJgyFXzKgY"
   },
   "outputs": [],
   "source": [
    "for j in tqdm(range(0,383)): \n",
    "  tqdm.write(f'   page{j+1}')\n",
    "  update_page = 50*j\n",
    "  for i in range(0,50):\n",
    "      f.html_parsing(f'tsv_files/page{j+1}/anime_{update_page+i+1}.tsv', \n",
    "                   f'./page_folders/page{j+1}/anime_{update_page+i+1}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d3c85-e80a-447b-b979-566270d54977",
   "metadata": {
    "id": "07Aqeywt7GlA"
   },
   "outputs": [],
   "source": [
    "with open('./tsv_files/header.tsv', 'wt') as out_file:\n",
    "    anime_tsv = csv.writer(out_file, delimiter='\\t')\n",
    "    anime_tsv.writerow(['animeTitle', 'animeType', 'animeNumEpisode', 'animeRelDate', 'animeEndDate', \n",
    "                        'animeMembers', 'animeScore', 'animeUsers', 'animeRank', 'animePopularity', 'animeSynopsis',\n",
    "                        'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c11cf8-8213-4c26-8046-7bed4dbff285",
   "metadata": {
    "id": "eJpAZJMd7tup"
   },
   "source": [
    "Once we collected all the tsv's file we have to clean them (replacing empty lists w/ empty strings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf31543-0897-427f-9eab-23a643c0215b",
   "metadata": {
    "id": "h7rNS--279qR"
   },
   "outputs": [],
   "source": [
    "for j in tqdm(range(0,383)): \n",
    "  tqdm.write(f'   page{j+1}')\n",
    "  update_page = 50*j\n",
    "  for i in range(0,50):\n",
    "        f = open(f'./tsv_files/page{j+1}/anime_{update_page+i+1}.tsv', mode = 'r', encoding='utf-8')\n",
    "        txt  = f.read().strip()\n",
    "        f.close()\n",
    "        f = open(f'./tsv_files/page{j+1}/anime_{update_page+i+1}.tsv', mode = 'w', encoding='utf-8')\n",
    "        f.write(txt.replace('\\\\', ''))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f11ca-330e-4d3e-9fe6-198e0dbbf5e0",
   "metadata": {},
   "source": [
    "Last step is to merge all the tsv in a single file called _tsv_merged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c845b-d9d0-4ca4-a908-c61c4b9f43d0",
   "metadata": {
    "id": "Suywi7MeikcU"
   },
   "outputs": [],
   "source": [
    "for j in tqdm(range(0,383)): \n",
    "  tqdm.write(f'   page{j+1}')\n",
    "  update_page = 50*j\n",
    "  for i in range(0,50):\n",
    "    copyfile(f'./tsv_files/page{j+1}/anime_{update_page+i+1}.tsv', \n",
    "                   f'./tsv_merged/anime_{update_page+i+1}.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2e919-0189-4a00-979e-a8ec124c3039",
   "metadata": {
    "id": "trz8B9S7vVte"
   },
   "outputs": [],
   "source": [
    "with open(f\"./tsv_files/header.tsv\", encoding='utf-8') as f:\n",
    "    header = f.read().strip().split('\\t')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e4a8a-09b0-4d24-bc55-87422e04ec36",
   "metadata": {},
   "source": [
    "## 2. Building the Search Engine\n",
    "\n",
    "The fundametal steps to build a search engine for documents are:\n",
    "\n",
    "1. Pre-processing of the documents ending up with a dictionary of the document id as key and the list of tokens of the document as values.\n",
    "\n",
    "2. Building the inverted index such thta for each token we have all the documents in which is contained. In the second part we'll consider as value a tuple of document and tfidf of the token for that document.\n",
    "\n",
    "3. Implementing a function thta given a query retrieves the related documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d10af-75d6-4db0-9a32-23c6471a6d02",
   "metadata": {},
   "source": [
    "The first step is to build a dictionary of dictionaries, for every anime we'll have a dictionary containing all the informations retrieved earlier. The query will be focused only on the synopsys for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c781d-16b8-4cc0-8eff-ae7682fd8793",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8JZQSlxWzZLX"
   },
   "outputs": [],
   "source": [
    "D = {} # this dictionary will store all the dictionaries of the single animes\n",
    "for i in tqdm(range(0,19126)):\n",
    "\n",
    "    d = f.Dict(f\"/content/drive/MyDrive/ADM-HW3/tsv_merged/anime_{i+1}.tsv\")\n",
    "    D[f\"anime_{i+1}\"] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d02a6-ea1f-4278-a9de-dd04cb9a73e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Synopsis preprocessing\n",
    "First, we must pre-process all the information collected for each anime by:\n",
    "\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "- Anything else we think it's needed\n",
    "\n",
    "For this purpose, we use the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ca9c0-6003-430d-a2ef-ed1c26715d2c",
   "metadata": {},
   "source": [
    "Once we preprocessed with the _text_preprocessing_ function we collect all the unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a8327-60ef-474a-8dae-f91988ff2b10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrkI5qdmwE-H",
    "outputId": "e6594a8b-be45-435c-bf45-3c69df23cc0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19125/19125 [01:46<00:00, 179.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# first we preprocess the synopsis by lowering the case removing stopwords, punctuation and numbers and finally stemming\n",
    "all_tokens = []\n",
    "\n",
    "for key in tqdm(D):\n",
    "\n",
    "    out, tkns = f.text_preprocessing(D[key]['animeSynopsis'], lower = True, numbers = True, stemming = True)\n",
    "\n",
    "    for token in tkns:\n",
    "        if token not in all_tokens: # we want to store all the unique tokens in a list\n",
    "          all_tokens.append(token)\n",
    "\n",
    "# save all tokens in a txt file\n",
    "f = open(f\"/content/drive/MyDrive/ADM-HW3/tokens.txt\", \"w\", encoding='utf-8')\n",
    "for i in all_tokens:\n",
    "    f.write(i + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c8629-ef1e-41a1-acc4-9bb555f5832d",
   "metadata": {},
   "source": [
    "We map each token to an integer in the following dictionary, this we'll be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6f6bd-98e4-4617-82f4-3a52b821d9bd",
   "metadata": {
    "id": "NByyQf5IpfJK"
   },
   "outputs": [],
   "source": [
    "# we map every unique token to an integer and store it in a dictionary\n",
    "term_id = range(1, len(all_tokens))\n",
    "vocab = {k: v for k,v in zip(all_tokens, term_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a5a53-f017-4df5-84b4-65181190a78c",
   "metadata": {},
   "source": [
    "Then we store in two arrays the tokens and the synopsis and we'll use them as input in the _inv_index_ function that will build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15076984-c36c-407c-9f33-dba3f2a495bd",
   "metadata": {
    "id": "Bq4G3W58UqYU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we store the tokens and the synopsis in two numpy arrays\n",
    "\n",
    "tok = list(f.vocab.keys())\n",
    "tok = np.array(tok)\n",
    "\n",
    "syn = []\n",
    "for key in tqdm(D):\n",
    "\n",
    "    a, b = f.text_preprocessing(D[key]['animeSynopsis'], lower = True, numbers = True, stemming = True)\n",
    "    if a == 'synopsi inform ad titl help improv databas ad synopsi': \n",
    "      syn.append([''])\n",
    "      f.preproc_D[key]['animeSynopsis'] = [' ']\n",
    "\n",
    "    else: \n",
    "      \n",
    "      f.preproc_D[key]['animeSynopsis'] = b\n",
    "\n",
    "\n",
    "syn = np.array(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e3a08-09c0-44b9-a92b-033f13258ee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaE7ozSWg0AO",
    "outputId": "1b8b0b4d-bea3-427c-8a3b-25e40f238c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43615/43615 [18:09<00:00, 40.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# we compute the inverted index\n",
    "\n",
    "inv_index = f.inverted_index(tok, syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e8208-4af7-4b70-872e-723df889f31a",
   "metadata": {},
   "source": [
    "### Executing the queries \n",
    "Given a query, that we let the user enter:\n",
    "\n",
    "*saiyan race*\n",
    "\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "**What documents do we want?**\n",
    "\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "- **animeTitle**\n",
    "- **animeDescription**\n",
    "- **Url**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1586c36b-6e3a-40f1-bbd0-c7c031d6be4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "executionInfo": {
     "elapsed": 3790,
     "status": "ok",
     "timestamp": 1640459744311,
     "user": {
      "displayName": "arturo ghinassi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02751271912569120406"
     },
     "user_tz": -60
    },
    "id": "tSUlXqucSnO0",
    "outputId": "fb6b84cb-bf4c-43fc-f9d4-b14698d272eb"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "search bar:  saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "1                                      Dragon Ball Z   \n",
       "2                           Dragon Ball Super: Broly   \n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "4                                    Dragon Ball Kai   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "1  Five years after winning the World Martial Art...   \n",
       "2  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "4  Five years after the events of Dragon Ball, ma...   \n",
       "\n",
       "                                                 Url  \n",
       "1  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  \n",
       "2  https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "4  https://myanimelist.net/anime/6033/Dragon_Ball...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a07e9-f2fa-4c47-b251-345875c1ab83",
   "metadata": {},
   "source": [
    "## Cosine similarity and Tf-idf\n",
    "\n",
    "At this point we would like to have a more powerful search engine that given a query will compute the cosine similarity and return the best ranked results by using a MinHeap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfc6b1-da2f-4d02-9ebe-b3a00e1743f0",
   "metadata": {},
   "source": [
    "Our second Inverted Index is of this format:\n",
    "\n",
    "{\n",
    "\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "\n",
    "...}\n",
    "\n",
    "Practically, for each word we want the list of documents in which it is contained in, and the relative *tfIdf* score.\n",
    "\n",
    "**Tip**: to compute the tfidf you can also use the sci-kit library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73f054b-400c-4458-8c70-6d3609032bd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAaahKSDqSyJ",
    "outputId": "3b3a2e8f-4465-4808-87fd-0af20abf2d88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19125/19125 [00:41<00:00, 463.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# this time we store the synopsis in a dicitionary\n",
    "\n",
    "anime_id, syn = [], []\n",
    "for key in tqdm(f.D):\n",
    "    anime_id.append(key)\n",
    "    a, b = f.text_preprocessing(f.D[key]['animeSynopsis'], lower = True, numbers = True, stemming = True)\n",
    "    syn.append(b)\n",
    "\n",
    "synopsis = {k:v for k,v in zip(anime_id, syn)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d06072-4ce4-4bdd-8be3-413da049cecc",
   "metadata": {},
   "source": [
    "Then we compute the tf-idf as follows:\n",
    "\n",
    "It is the combination of *Term Frequency (TF)* and *Inverse Data Frequency (IDF)*.\n",
    "\n",
    "TF is the number of times a word *t* appears in a document *d* divided by the total number of words in the document. Every document has its own term frequency:\n",
    "\n",
    "$tf_{t,d}=\\frac{n_{t,d}}{\\sum_{t'\\in d} n_{t',d}}$\n",
    "\n",
    "\n",
    "\n",
    "The IDF is a measure of how much information the word provides, i.e. if it's common or rare across all documents.\n",
    "IDF is the log of the number of all documents *N* divided by the number of documents *d* that contain the word *t*. IDF determines the weight of rare words across all documents in the corpus:\n",
    "\n",
    "$idf(t,D)=\\log \\left(\\frac{N}{| \\{ d\\in D: t\\in D \\} | }\\right)$\n",
    "\n",
    "TF-IDF is given by the multiplication of TF and IDF:\n",
    "\n",
    "$w_{t,d,D}=tf_{t,d} \\times idf(t,D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f917c-2afa-4a64-9b2f-36ffe38bb5d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Bth-8mltXva",
    "outputId": "f520959e-c77b-41c8-9f73-8422a2bca742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43615/43615 [00:04<00:00, 10526.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tok = list(f.vocab.keys())\n",
    "tok = np.array(tok)\n",
    "\n",
    "for j in tqdm(range(0, len(tok))):\n",
    "    term_j,  k = tok[j], 0\n",
    "\n",
    "    for i in f.inv_index[term_j]:\n",
    "        try:\n",
    "            doc_i = synopsis[i]\n",
    "            tfidf = ( doc_i.count(term_j) / len(doc_i) ) * ( np.log10( len(synopsis) / len(f.inv_index[term_j]) ))\n",
    "            f.inv_index[term_j][k]  = (i, tfidf)\n",
    "            k += 1\n",
    "        except KeyError: pass\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba60ac-9441-4ad5-a8b5-09ec20b584c2",
   "metadata": {},
   "source": [
    "### Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9ad27f-4758-4335-8bd1-665a03f47861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 4221,
     "status": "ok",
     "timestamp": 1640461623170,
     "user": {
      "displayName": "arturo ghinassi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02751271912569120406"
     },
     "user_tz": -60
    },
    "id": "gqtU9iP13PF4",
    "outputId": "ed3c195f-c34e-4168-f3bb-387e5438e4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search bar: alchemist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8608d590-b0a7-4374-8651-9bca5837a594\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shinmai Renkinjutsushi no Tenpo Keiei</td>\n",
       "      <td>Shoot for the stars! I'm going to be the count...</td>\n",
       "      <td>https://myanimelist.net/anime/49849/Shinmai_Re...</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arcana Famiglia: Capriccio - stile Arcana Fami...</td>\n",
       "      <td>After toiling away in his lab, the alchemist J...</td>\n",
       "      <td>https://myanimelist.net/anime/15411/Arcana_Fam...</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ulysses: Jehanne Darc to Renkin no Kishi</td>\n",
       "      <td>The story is set in the 15th century, during t...</td>\n",
       "      <td>https://myanimelist.net/anime/36510/Ulysses__J...</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8608d590-b0a7-4374-8651-9bca5837a594')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8608d590-b0a7-4374-8651-9bca5837a594 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8608d590-b0a7-4374-8651-9bca5837a594');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                          animeTitle  ... Similarity\n",
       "1              Shinmai Renkinjutsushi no Tenpo Keiei  ...       0.76\n",
       "2  Arcana Famiglia: Capriccio - stile Arcana Fami...  ...       0.14\n",
       "3           Ulysses: Jehanne Darc to Renkin no Kishi  ...       0.13\n",
       "\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.search_cosine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7311bc-01df-4f67-a449-c5987b55f1e0",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This is a Naive search engine that could be imporved for example by using hash functions to do the queries or just changing the way a query is computed adding filters for example or just defining a more discriminative score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
